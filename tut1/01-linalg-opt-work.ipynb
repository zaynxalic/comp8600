{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will practice minimising a cost function with gradient descent.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Linear algebra (see Sam Roweis' notes, linked below, for matrix calculus tips)\n",
    "- Python programming\n",
    "- Preferably: Using numpy for matrix calculations (precourse material)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Using numpy ndarrays for matrix calculations\n",
    "- Using scipy.optimise routines to minimise a cost function, with and without a gradient\n",
    "- Randomly generating input values for testing\n",
    "\n",
    "## Pre-lab notes\n",
    "In this lab, you will apply linear algebra to to minimise a cost function in three steps: implementing the cost function, implementing a gradient function, and applying gradient descent. We will be doing this to solve problems throughout the course.\n",
    "\n",
    "As in all labs, feel free to skip questions if you get stuck, and ask your tutor if you have any questions!\n",
    "\n",
    "A note on style: in this course we emphasise *functional decomposition* in code style. Avoid using global variables, and remember that often splitting code off into separate functions can make it more readable and testable. (Jupyter notebooks let you call functions defined in previous cells.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment (this cell contains Latex macros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an *optimisation problem*, we have a real-valued function $f(X)$ and aim to find an $X$ that minimises/maximises $f$. In machine learning, $f$ is typically a *cost function* or *loss function*, which we want to minimise. This function might measure the error between the indicator values our model predicts and the true values in the training data.\n",
    "\n",
    "In this lab, we will work with the following toy example. Let's minimise $f: \\RR^{n \\times p} \\rightarrow \\RR$, defined as  \n",
    "\\begin{equation*}\n",
    "f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F,\n",
    "\\end{equation*}\n",
    "where $C$ is a fixed $n \\times n$ symmetric matrix, $\\mu$ is a scalar larger than the $p^{th}$ smallest eigenvalue of $C$, and $N$ is a $p \\times p$ diagonal matrix with distinct positive entries on the diagonal. We also assume that $n \\geq p$.\n",
    "\n",
    "Let's explain some details of the above you might find unfamiliar.\n",
    "\n",
    "- $f$ maps an $n \\times p$ matrix $X$ to a number on the real line.\n",
    "- For a square matrix $A$, we write $\\trace{A}$ for its trace, which is the sum of all elements on the diagonal of $A$.\n",
    "- For any matrix $A \\in \\RR^{n \\times p}$, define its *Frobenius norm* as $\\Norm{A}_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{p} A_{i,j}^2}$. Can you show that $\\Norm{A}_F = \\sqrt{\\trace{A^T A}} = \\sqrt{\\trace{A A^T}}$ as well?\n",
    "- In the definition above, note that only $X$ is the input to $f$; everything else, including $C$, $\\mu$ and $N$, are given to us. Therefore, minimising $f$ only involves finding $X$ while keeping everything else fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $. You can choose any formula above.\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```?\n",
    "2. Given $n \\geq p$, what formula do you think gives the fastest implementation?\n",
    "\n",
    "### Notes\n",
    "You can use the following syntax in `numpy`:\n",
    "- To get the transpose of a matrix `A`, use `A.T`.\n",
    "- To multiply two matrices `A` and `B`, use `A @ B`.\n",
    "- To take the sum of all elements of a matrix `A`, use `np.sum(M)` or just `M.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def frobenius_norm(x):\n",
    "    x = np.sqrt(np.trace(x@x.T))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code to test your ```frobenius_norm``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your frobenius_norm: 2.6918\n",
      "True frobenius_norm: 2.6918\n"
     ]
    }
   ],
   "source": [
    "# Test for frobenius_norm\n",
    "M = np.array([[0.60094641, 0.9759039 , 0.85339979],\n",
    "              [0.73835924, 0.34727218, 0.01618439],\n",
    "              [0.83347728, 0.81740037, 0.36525059],\n",
    "              [0.62000774, 0.75117202, 0.93941705],\n",
    "              [0.88817543, 0.37140933, 0.5327329 ]])\n",
    "print(f'Your frobenius_norm: {frobenius_norm(M):.4f}')\n",
    "print(f'True frobenius_norm: 2.6918')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Implementing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function, ```cost_function_for_matrix```, which implements the above cost function $f(X)$ defined by\n",
    "\\begin{equation*}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation*}\n",
    "for $ X \\in \\RR^{n \\times p} $, $ n \\ge p $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def cost_function_for_matrix(X,C,N,mu):\n",
    "    f_x = 1/2 * np.trace(X.T@C@X@N) + mu*1/4*frobenius_norm(N-X.T@X)**2\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code to test your ```cost_function_for_matrix``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your cost_function_for_matrix: 2.0435\n",
      "True cost_function_for_matrix: 2.0435\n"
     ]
    }
   ],
   "source": [
    "# Test for cost_function_for_matrix\n",
    "X = np.array([[0.09325036, 0.15792007, 0.43645094, 0.95070763, 0.03097754],\n",
    "              [0.73160312, 0.83306319, 0.02238594, 0.51079686, 0.00742748],\n",
    "              [0.72548058, 0.80074044, 0.0988811 , 0.28356641, 0.91609969]])\n",
    "C = np.array([[0.39507301, 0.14470985, 0.29870771],\n",
    "              [0.14470985, 0.10065113, 0.34081829],\n",
    "              [0.29870771, 0.34081829, 0.83439717]])\n",
    "N = np.diag([0.95854111, 0.77966088, 0.18859065, 0.9348394, 0.6822931])\n",
    "mu = 0.2358\n",
    "print(f'Your cost_function_for_matrix: {cost_function_for_matrix(X, C, N, mu):.4f}')\n",
    "print(f'True cost_function_for_matrix: 2.0435')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Cost function with vector argument\n",
    "\n",
    "The standard optimisation functions we will be using work only for cost functions that take a vector as the varying argument. Write a new function, ```cost_function_for_vector```, that takes $X$ represented as a vector of length $np$ rather than a matrix of dimensions $n\\times p$. What arguments will this function take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def cost_function_for_vector(X,C,N,mu, n, p):\n",
    "    X = X.reshape(n,p)\n",
    "    f_x = 1/2 * np.trace(X.T@C@X@N) + mu*1/4*frobenius_norm(N-X.T@X)**2\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code to test your ```cost_function_for_vector``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your cost_function_for_matrix: 2.0435\n",
      "True cost_function_for_matrix: 2.0435\n"
     ]
    }
   ],
   "source": [
    "# Test for cost_function_for_vector\n",
    "n = 3\n",
    "p = 5\n",
    "# Note that X is a one-dimensional array\n",
    "X = np.array([0.09325036, 0.15792007, 0.43645094, 0.95070763, 0.03097754, \n",
    "              0.73160312, 0.83306319, 0.02238594, 0.51079686, 0.00742748,\n",
    "              0.72548058, 0.80074044, 0.0988811 , 0.28356641, 0.91609969])\n",
    "C = np.array([[0.39507301, 0.14470985, 0.29870771],\n",
    "              [0.14470985, 0.10065113, 0.34081829],\n",
    "              [0.29870771, 0.34081829, 0.83439717]])\n",
    "N = np.diag([0.95854111, 0.77966088, 0.18859065, 0.9348394, 0.6822931])\n",
    "mu = 0.2358\n",
    "print(f'Your cost_function_for_matrix: {cost_function_for_vector(X, C, N, mu, n, p):.4f}')\n",
    "print(f'True cost_function_for_matrix: 2.0435')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Minimising the cost function\n",
    "\n",
    "We will be using two minimisation methods from the ``scipy.optimize`` module.\n",
    "- First, we will use ``fmin``, which takes an initial guess for $X$ and the function $f$.\n",
    "- Second, we will use ``fmin_bfgs``, which takes $X$, $f$ and the gradient of $f$. We will write a function to compute the gradient later on.\n",
    "\n",
    "Since ``fmin_bfgs`` uses additional information bout the function, we should expect to have substantially faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4a. Minimising with ```fmin```\n",
    "\n",
    "Implement a function ```minimise_f_using_fmin``` that, for given values of $C$, $N$ and $\\mu$, finds the matrix $X$ that minimizes $f(X)$ using ``fmin``. You will likely need [the docs for ``fmin``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html). Check if your function converges for some (randomly generated) values of $C$, $N$ and $\\mu$.\n",
    "\n",
    "Summary of the docs: if you have a cost function $g(x, y)$ with a fixed value of $y$ and wish to find the value of $x$ that minimises it, the syntax for calling ``fmin`` would be ``fmin(g, x0, args=(y))`` where ``x0`` is an initial guess for the value of $x$, and ``args=(y)`` gives ``fmin`` the rest of the values to pass to the cost function. Note that it is necessary that the variable that can change is the first argument to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "?opt.fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimise_f_using_fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for minimise_f_using_fmin\n",
    "def randomly_generate_values(n, p):\n",
    "    C = np.random.rand(n,n)\n",
    "    C = C + C.T\n",
    "    N = np.diag(np.random.rand(p))\n",
    "    mu = frobenius_norm(C)\n",
    "    X0 = np.random.randn(n,p)\n",
    "    return C, N, mu, X0\n",
    "\n",
    "n = 3\n",
    "p = 2\n",
    "C, N, mu, X0 = randomly_generate_values(n, p)\n",
    "minimise_f_using_fmin(C, N, mu, n, p, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4b. Calculating the gradient of the cost function\n",
    "\n",
    "To use ``fmin_bfgs``, which is substantially more time efficient, we need to compute the gradient of $f(X)$ with respect to $X$. \n",
    "1. Derive a formula for the derivative of $f$ with respect to $X$, $\\frac{\\partial f}{\\partial X}$.\n",
    "2. Implement a function ``gradient_for_matrix`` that returns the gradient.\n",
    "3. Implement a function ``gradient_for_vector`` that returns the gradient, then flattens it to an array for use in ``fmin_bfgs``. Remember that this function should take $n$ and $p$ as inputs as well.\n",
    "\n",
    "You may want to use Sam Roweis' [Matrix Identities](https://cs.nyu.edu/~roweis/notes/matrixid.pdf) and/or the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) as a reference for matrix calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4c. Minimising the cost function using the gradient\n",
    "\n",
    "Write a function ```minimise_f_using_fmin_bfgs``` to minimise $f(X)$ using ```fmin_bfgs```. Have a look at the docs to find the correct syntax. Again, have a try of your function to check that it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for minimise_f_using_fmin_bfgs\n",
    "n = 3\n",
    "p = 2\n",
    "C, N, mu, X0 = randomly_generate_values(n, p)\n",
    "minimise_f_using_fmin_bfgs(C, N, mu, n, p, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Time for convergence\n",
    "\n",
    "We wish to check whether ``fmin_bfgs`` is actually faster than ``fmin``.\n",
    "- The most direct way to compare the performance of two algorithms is to compare their wall-clock running time on the same instance.\n",
    "- Another way is to compare the number of iterations needed to achieve some precision. You can read this off the summary of each algorithm above. For example, for ``fmin_bfgs``, ``Iterations: 295`` means it took ``fmin_bfgs`` 295 iterations until it converged.\n",
    "\n",
    "We will now compare ``fmin_bfgs`` and ``fmin`` on two instances: one in low-dimensional data and one in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5a. Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "The following function takes in an array ``E`` of $n$ numbers and produces a random $n \\times n$ matrix with the same eigenvalues as the numbers in ``E``. It uses a method in linear algebra called [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition).\n",
    "1. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "2. Can you explain why the following function actually gives you the correct output (a random matrix whose eigenvalues are all in ``E``)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_matrix_from_eigenvalues(E):\n",
    "    \"\"\"Create a square random matrix with a given set of eigenvalues\n",
    "    E -- list of eigenvalues\n",
    "    return the random matrix\n",
    "    \"\"\"\n",
    "    n    = len(E)\n",
    "    # Create a random orthogonal matrix Q via QR decomposition\n",
    "    # of a random matrix A\n",
    "    A    = np.random.rand(n,n)\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    #  similarity transformation with orthogonal\n",
    "    #  matrix leaves eigenvalues intact\n",
    "    return Q @ np.diag(E) @ Q.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5b. Checking convergence time\n",
    "\n",
    "Is ``fmin_bfgs`` actually faster than ``fmin``? Use the below code to compare runtimes.\n",
    "\n",
    "Report the runtimes and number of iterations for each algorithm for each instance. Do you see that ``fmin_bfgs`` improves from ``fmin``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_low_dimensional_data():\n",
    "    \"\"\"Initialise the data, low dimensions\"\"\"\n",
    "    n  = 3\n",
    "    p  = 2\n",
    "    mu = 2.7\n",
    "\n",
    "    N  = np.diag([2.5, 1.5])\n",
    "    E  = [1, 2, 3]\n",
    "    C  = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def initialise_higher_dimensional_data():\n",
    "    \"\"\"Initialise the data, higher dimensions\"\"\"\n",
    "    n  = 20\n",
    "    p  = 5\n",
    "    mu = p + 0.5\n",
    "\n",
    "    N  = np.diag(np.arange(p, 0, -1))\n",
    "    E  = np.arange(1, n+1)\n",
    "    C  = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "def pretty_printing(task_string):\n",
    "    line_length   = 76\n",
    "    spaces        = 2\n",
    "    left_padding  = (line_length - len(task_string)) // 2\n",
    "    right_padding = line_length - left_padding - len(task_string)\n",
    "    print(\"=\" * line_length)\n",
    "    print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "            \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "    print(\"=\" * line_length)    \n",
    "\n",
    "def run_and_time_all_tests():\n",
    "    \"\"\"Run all test and time them using a list of function names\"\"\"\n",
    "    List_of_Test_Names = [\"minimise_f_using_fmin\",\n",
    "                 \"minimise_f_using_fmin_bfgs\"]\n",
    "\n",
    "    List_of_Initialisations = [\"initialise_low_dimensional_data\",\n",
    "                               \"initialise_higher_dimensional_data\"]\n",
    "\n",
    "    for test_name in List_of_Test_Names:\n",
    "        for init_routine in List_of_Initialisations:\n",
    "            task_string  = test_name + \"(\" + init_routine + \")\"\n",
    "            pretty_printing(task_string)\n",
    "\n",
    "            start = time.time()\n",
    "            C, N, mu, n, p, X0 = globals()[init_routine]()\n",
    "            exec(test_name+\"(C,N,mu,n,p,X0)\")\n",
    "            run_time = time.time() - start\n",
    "            print(\"run_time :\", run_time)\n",
    "\n",
    "np.random.seed(42)\n",
    "run_and_time_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Minima of $f(X)$\n",
    "\n",
    "Use the below code to compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation*}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation*}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n",
    "\n",
    "What do you believe this means about $f(X)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(A):\n",
    "    \"\"\"Normalise the columns of a 2-D array or matrix to length one\n",
    "    A - array or matrix (which will be modified)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"A is not a 2-D array\")\n",
    "\n",
    "    number_of_columns = A.shape[1]\n",
    "    for i in range(number_of_columns):\n",
    "        A[:,i] /= np.linalg.norm(A[:,i], ord=2)\n",
    "\n",
    "\n",
    "def show_results(X_at_min, C):\n",
    "    \"\"\"Display the found arg min and compare with eigenvalues of C\n",
    "    X_at_min -- arguement at minimum found\n",
    "    C        -- symmetric matrix\n",
    "    \"\"\"\n",
    "    n,p = X_at_min.shape\n",
    "\n",
    "    normalize_columns(X_at_min)\n",
    "\n",
    "    # Get the eigenvectors belonging to the smallest eigenvalues\n",
    "    Eigen_Values, Eigen_Vectors = np.linalg.eig(C)\n",
    "    Permutation = Eigen_Values.argsort()\n",
    "    Smallest_Eigenvectors = Eigen_Vectors[:, Permutation[:p]]\n",
    "\n",
    "    if n < 10:\n",
    "        print(\"X_at_min:\")\n",
    "        print(X_at_min)\n",
    "        print()\n",
    "        print(\"Smallest_Eigenvectors:\")\n",
    "        print(Smallest_Eigenvectors)\n",
    "        print()\n",
    "    else:\n",
    "        Project_into_Eigenvectorspace = \\\n",
    "          Smallest_Eigenvectors * Smallest_Eigenvectors.T * X_at_min\n",
    "        Normal_Component = X_at_min - Project_into_Eigenvectorspace\n",
    "\n",
    "        print(\"norm(Normal_Component)/per entry :\", \\\n",
    "            np.linalg.norm(Normal_Component, ord=2) / float(n*p))\n",
    "\n",
    "def show_comparision(num=3):\n",
    "    for _ in range(num):\n",
    "        pretty_printing(\"Comparing X_at_min and Eigenvalues for random values\")\n",
    "        C, N, mu, n, p, X0 = initialise_low_dimensional_data()\n",
    "        X_at_min = minimise_f_using_fmin_bfgs(C,N,mu,n,p,X0)\n",
    "        show_results(X_at_min, C)\n",
    "\n",
    "show_comparision()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
