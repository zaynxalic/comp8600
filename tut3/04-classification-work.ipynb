{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will build, train, and test a logistic regression classifier.\n",
    "\n",
    "### Assumed knowledge:\n",
    "\n",
    "- Optimisation in Python (lab)\n",
    "- Regression (lab)\n",
    "- Binary classification with logistic regression (lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "\n",
    "- Implementing logistic regression\n",
    "- Practical binary classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will be working with the census-income dataset, which shows income levels for people in the 1994 US Census. We will predict whether a person has $\\leq \\$50000$ or $> \\$50000$ income per year.\n",
    "\n",
    "The data are included with this notebook as `04-dataset.tsv`, a textfile where in each row of data, the individual entries are delimited by tab characters. Download the data from the [course website](https://machlearn.gitlab.io/sml2020/tutorials/04-dataset.tsv)\n",
    "Load the data into a NumPy array called `data` using `numpy.genfromtxt`:\n",
    "\n",
    "```python\n",
    "    numpy.genfromtxt(filename)\n",
    "```\n",
    "\n",
    "The column names are given in the variable `columns` below.\n",
    "The `income` column are the targets, and the other columns will form our data used to try and guess the `income`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = np.genfromtxt(\"04-dataset.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap - Binary classification\n",
    "\n",
    "The idea behind this lab is that for each person, we want to\n",
    "try and predict if their income is above the threshold of $\\$50,000$ or not,\n",
    "based on a series of other data about their person: `age, education,...`.\n",
    "\n",
    "As per usual, for the $n^\\text{th}$ row, the first entry is the target $t_n$, and the rest\n",
    "forms the data vector $\\mathbf{x}_n$.\n",
    "\n",
    "We have two classes, $C_1$ representing the class of $ <\\$ 50,000$, which corresponds to\n",
    "a target of $t_n = 0$, and $C_2$, representing the class of $ >\\$50,000$, corresponding to\n",
    "a target of $t_n = 1$. Our objective is to learn a discriminative function $f_{\\mathbf{w}}(\\mathbf{x})$,\n",
    "parametrised by a weight vector $\\mathbf{w}$ that\n",
    "predicts which income class the person is in, based on the data given.\n",
    "\n",
    "We assume that each piece of information $(t_n, \\mathbf{x}_n)$ is i.i.d, and\n",
    "that there is some hidden probability distribution from which these target/data points are drawn.\n",
    "We will construct a likelihood function that indicates \"What is the likelihood of this particular\n",
    "weight vector $\\mathbf{w}$ having generated the observed training data $\\left\\{(t_n, \\mathbf{x}_n)\\right\\}_{n=1}^N$\".\n",
    "\n",
    "## Recap - Feature map, basis function\n",
    "\n",
    "Now some classes are not linearly seperable (we cannot draw a line such that all of one class is on one side,\n",
    "and all of the other class is on the other side). But by applying many fixed non-linear \n",
    "transformations to the inputs $\\mathbf{x}_n$ first, for some suitable choice\n",
    "of transformation $\\phi$ the result will usually be linearly separable\n",
    "(See week 3, pg 342 of the lecture slides).\n",
    "\n",
    "We let\n",
    "$$\n",
    "\\mathbf{\\phi}_n := \\phi(\\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "and work in this feature space rather than the input space.\n",
    "For the case of two classes, we could guess that the target is a linear combination of the features,\n",
    "$$\n",
    "\\hat{t}_n = \\mathbf{w}^T \\mathbf{\\phi}_n\n",
    "$$\n",
    "but $\\mathbf{w}^T \\mathbf{\\phi}_n$ is a real number, and we want $\\hat{t}_n \\in \\{0,1\\}$.\n",
    "We could threshold the result,\n",
    "$$\n",
    "\\hat{t}_n =\n",
    "\\begin{cases}\n",
    "1 & \\mathbf{w}^T \\mathbf{\\phi}_n \\geq 0 \\\\\n",
    "0 & \\mathbf{w}^T \\mathbf{\\phi}_n < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "but the discontinuity makes it impossible to define a sensible gradient. \n",
    "\n",
    "## Recap - Logistic Regression\n",
    "\n",
    "(We assume that the classes are already linearly seperable, and use our input space as our feature space.\n",
    "We also assume the data is i.i.d).\n",
    "\n",
    "Instead of using a hard threshold like above, in logistic regression\n",
    "we can use the sigmoid function $\\sigma(a)$\n",
    "$$\n",
    "\\sigma(a) := \\frac{1}{1 + e^{-a}}\n",
    "$$\n",
    "which has the intended effect of \"squishing\" the real line to the interval $[0,1]$.\n",
    "This gives a smooth version of the threshold function above, that we can differentiate.\n",
    "The numbers it returns can be interpreted as a probability of the estimated target $\\hat{t}$ belonging\n",
    "to a class $C_i$ given the element $\\phi$ of feature space. In the case of two classes, we define\n",
    "\n",
    "\\begin{align}\n",
    "p(C_1 | \\phi ) &:= \\sigma (\\mathbf{w}^T \\phi_n) = y_n \\\\\n",
    "p(C_2 | \\phi ) &:= 1 - p(C_1 | \\phi_n)= 1- y_n\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The likelihood function $p(\\mathbf{t} | \\mathbf{w}, \\mathbf{x})$ is what we want to maximise as a function\n",
    "of $\\mathbf{w}$. Since $\\mathbf{x}$ is fixed, we usually write the likelihood function as $p(\\mathbf{t} | \\mathbf{w})$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{t} | \\mathbf{w})\n",
    "&= \\prod_{n=1}^N p(t_n | \\mathbf{w}) \\\\\n",
    "&= \\prod_{n=1}^N \n",
    "\\begin{cases}\n",
    "p(C_1 | \\phi_n) & t_n = 1 \\\\\n",
    "p(C_2 | \\phi_n) & t_n = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "Note that\n",
    "$$\n",
    "\\begin{cases}\n",
    " y_n & t_n = 1 \\\\\n",
    "1 - y_n & t_n = 0\n",
    "\\end{cases}\n",
    "= y_n^{t_n} (1-y_n)^{1-t_n}\n",
    "$$\n",
    "as if $t_n = 1$, then $y_n^1 (1-y_n)^{1-1} = y_n$ and if $t_n = 0$ then $y_n^0 (1-y_n)^{1-0} = 1-y_n$.\n",
    "This is why we use the strange encoding of $t_n=0$ corresponds to $C_2$ and $t_n=1$ corresponds to $C_1$.\n",
    "Hence, our likelihood function is \n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{w}) = \\prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n}, \\quad y_n = \\sigma(\\mathbf{w}^T \\phi_n)\n",
    "$$\n",
    "This function is quite unpleasant to try and differentiate, but we note that $p(\\mathbf{t} | \\mathbf{w})$\n",
    "is maximised when $\\log p(\\mathbf{t} | \\mathbf{w})$ is maximised.\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{t} | \\mathbf{w}) \n",
    "&= \\log \\prod_{n=1}^N y_n^{t_n} (1-y_n)^{1-t_n} \\\\\n",
    "&= \\sum_{n=1}^N \\log \\left( y_n^{t_n} (1-y_n)^{1-t_n} \\right) \\\\\n",
    "&= \\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "\\end{align}\n",
    "Which is maximised when $- \\log p(\\mathbf{t} | \\mathbf{w})$ is minimised, giving us our error function.\n",
    "$$\n",
    "E(\\mathbf{w}) := - \\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "$$\n",
    "We can then take the derivative of this, which gives us\n",
    "$$\n",
    "\\nabla_\\mathbf{w} E(\\mathbf{w}) = \\sum_{n=1}^N (y_n - t_n) \\phi_n\n",
    "$$\n",
    "\n",
    "(Note: We also usually divide the error by the number of data points, to obtain the average error. The error\n",
    "shouldn't get 10 times as large just because there is more data avaliable, so we should divide by the\n",
    "number of error points to reflect that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Derivative of binary cross entropy\n",
    "Take the derivative of $E(\\mathbf{w})$, and show that it is equal to the above. Note that the derivative doesn't have any sigmoid functions. (Hint: Use the identity $\\sigma'(a) = \\sigma(a) \\left( 1- \\sigma(a) \\right)$ to simplify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start your solution here (you can convert this block to markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. $L_2$ regularisation, Gaussian prior\n",
    "\n",
    "Now we consider an isotropic guassian prior for $\\mathbf{w}$ (i.e. $w \\sim \\mathcal{N}(\\mathbf{0},\\alpha^{-1}I) $). Use the likelihood we derived above and the Gaussian prior, show that the error function could be writen as $$ E(\\mathbf{w}) = - \\left(\\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\\right) + \\frac{\\lambda}{2}\\Vert \\mathbf{w} \\Vert_2^2$$ for some $\\lambda$. Write out the relation between $\\lambda$ and $\\alpha$.\n",
    "\n",
    "Hint: Derive the negative logarithm of posterior $p(\\mathbf{w}|\\mathbf{t})$ and discard the constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start your solution here (you can convert this block to markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Derivative of binary cross entropy with regularisation\n",
    "Take the derivative of $E(\\mathbf{w})$ again, accounting for the added regularisation term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start your solution here (you can convert this block to markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Classification with logistic regression\n",
    "\n",
    "Implement binary classification using logistic regression and $L_2$ regularisation. Make sure you write good quality code with comments and docstrings where appropriate. In this question, we use the mean error function defined above.\n",
    "\n",
    "$$ E(\\mathbf{w}) = \\frac{1}{N}\\left\\{-\\left(\\sum_{n=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right)\\right) + \\frac{\\lambda}{2}\\Vert \\mathbf{w} \\Vert_2^2\\right\\}$$\n",
    "\n",
    "To optimise your cost function, we will implement a stochastic gradient descent algorithm by hand. We first recall that, in (full-batch) gradient descent, the iteration formula is given by $$ \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\eta\\nabla_{\\mathbf{w}}E(\\mathbf{w}_k),$$ where $\\eta$ is the learning rate. For stochatsic gradient descent, instead of using the full dataset in each iteration, we will divide the dataset into several mini-batches and use the gradient with respect to one mini-batch to update the parameter in each iteration. Specificly, we first write our regulariser into the sum, \n",
    "$$ E(\\mathbf{w}) = \\frac{1}{N}\\sum_{n=1}^N \\left\\{ -\\left(t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right) + \\frac{\\lambda}{2N}\\Vert \\mathbf{w} \\Vert_2^2 \\right\\}.$$ Then, for a minibatch $\\mathcal{B}_i$ (suppose $\\mathcal{B}_i$ is a set of indices), the stochastic gradient $g_{\\mathcal{B}_i}$ could be defined as \n",
    "$$ g_{\\mathcal{B}_i}(\\mathbf{w}_k) = \\frac{1}{N_{\\mathcal{B}_i}}\\nabla_\\mathbf{w}\\left\\{\\sum_{n\\in\\mathcal{B}_i} \\left\\{ -\\left(t_n \\log y_n +  (1-t_n) \\log (1-y_n)\\right) + \\frac{\\lambda}{2N}\\Vert \\mathbf{w} \\Vert_2^2 \\right\\}\\right\\}\\Bigg\\vert_{\\mathbf{w} = \\mathbf{w}_k}.$$ In each step, the updation formula is given by $$ \\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\eta g_{\\mathcal{B}_i}(\\mathbf{w}_k).$$ Note that, for each iteration, we can choose a mini-batch in turn.\n",
    "\n",
    "\n",
    "By above equations, implement five functions:\n",
    "\n",
    "- `cost(w, X, t, a, N)`, which calculates the value of the cost function in a mini-batch,\n",
    "- `grad(w, X, t, a, N)`, which calculates the (stochastic) gradient of the cost function in a mini-batch,\n",
    "- `create_mini_batches(X_train, t_train, num_batches)`, which creates a list of mini-batch,\n",
    "- `train(X_train, t_train, a, learning_rate, num_iterations, num_batches)`, which returns the maximum likelihood weight vector using stochastic gradient desecent, and\n",
    "- `predict(w, X)`, which returns predicted class probabilities,\n",
    "\n",
    "where \n",
    "* $\\mathbf{w}$ is a weight vector, \n",
    "* $X$ is a matrix of examples, \n",
    "* $t$ is a vector of labels/targets, \n",
    "* $a$ is the regularisation weight. \n",
    "\n",
    "(We would use $\\lambda$ for the regularisation term, but `a` is easier to type than `lambda`, and\n",
    "`lambda` is a reserved keyword in python, for lambda functions).\n",
    "\n",
    "See below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the bias term.\n",
    "\n",
    "## Note\n",
    "\n",
    "* You should use 80% of the data as your training set, and 20% of the data as your test set.\n",
    "* You also may want to normalise the data before hand. If the magnitude of $\\mathbf{w}^T \\phi_n$\n",
    "is very large, the gradient of $\\sigma(\\mathbf{w}^T \\phi_n)$ will be very near zero, which can\n",
    "cause convergence issues during numerical minimisation. If each element in a particular column is\n",
    "multiplied by a scalar (say, all elements of the `age` column) then the result is essentially the same\n",
    "as stretching the space in which the data lives. The model will also be proportionally stretched,\n",
    "but will not fundamentally change the behaviour. So by normalising each column, we can avoid\n",
    "issues related to numerical convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000e+00 3.900e+01 1.300e+01 0.000e+00 0.000e+00 2.174e+03 0.000e+00\n",
      " 4.000e+01 1.000e+00]\n"
     ]
    }
   ],
   "source": [
    "assert data_raw.shape[1] == len(columns)\n",
    "data = np.concatenate([data_raw, np.ones((data_raw.shape[0], 1))], axis=1)  # add a column of ones\n",
    "data.shape\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4a. Define the loss and gradient\n",
    "Implement sigmoid function, binary cross entropy and the (stochastic) gradient of the error function as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2689414213699951\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(a):\n",
    "    return 1/(1+math.e**-a)\n",
    "\n",
    "def cost(w, X, t, a, N): # N is the total sample size\n",
    "    # 注意dimension\n",
    "    y_n = w.T@X\n",
    "    every_cost = -(t@np.log())\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def stochastic_grad(w, X, t, a, N, N_batch): # N is the total sample size\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def batch_grad(w, X, t, a, N): # N is the total sample size\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4b. Divide mini-batches\n",
    "Given the training set and the number of minibatches we want, implememt the function `create_mini_batches` which will return a list of tuples such that each tuple represents a mini-batch which contains features and corresponding targets. (i.e. the output should be $[(X_1,t_1), (X_2,t_2), \\ldots, (X_{num\\_batches},t_{num\\_batches})])$\n",
    "\n",
    "Hint: Use `np.random.shuffle()` to shuffle the dataset first. Make sure training data are spread as evenly as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for splitting data into train and test with ratio 80/20, DO NOT CHANGE\n",
    "\n",
    "N = np.shape(data)[0]\n",
    "num_train = int(0.8*N)\n",
    "t_train = data[:num_train,0]\n",
    "t_test = data[num_train:, 0]\n",
    "#32561\n",
    "norm_data = data[:, 1:] / data[:, 1:].sum(axis=0, keepdims=True)\n",
    "X_train = norm_data[:num_train, :]\n",
    "X_test = norm_data[num_train:, :]\n",
    "assert X_test.shape[1] == len(columns)\n",
    "x_columns = columns[1:] + ['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X_train, t_train, num_batches):\n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4c. Train the model\n",
    "Implement the function `train(mini_batches, a, learning_rate, num_iterations)` which returns the maximum likelihood weight vector using stochastic gradient desecent. You can tune your `learning_rate` and `num_iterations` to attain better performance. \n",
    "\n",
    "Hint: You can try to plot the loss-iteration curve to make sure your algorithm converge properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6eedd57cdc3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a is regularisation parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Keep track of loss value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss_his\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(X_train, t_train, a, learning_rate, num_iterations, num_batches):  # a is regularisation parameter\n",
    "    \n",
    "    # Keep track of loss value\n",
    "    loss_his = []\n",
    "    \n",
    "    # TODO: Create mini-batches\n",
    "    mini_batches = None\n",
    "    \n",
    "    # TODO: Initialise parameter\n",
    "    w = None \n",
    "    \n",
    "    # Train\n",
    "    for i in range(num_iterations):\n",
    "        # TODO: Update your parameter w (perform SGD now, expecting multiple lines of codes here)\n",
    "        \n",
    "        # Keep track of loss, no need to change\n",
    "        \n",
    "        loss = cost(w, X_train, t_train, a, X_train.shape[0])\n",
    "        loss_his.append(loss)\n",
    "        \n",
    "        # print the loss per 1000 iters, no need to change\n",
    "        if np.mod(i,1000) == 0:\n",
    "            print(\"*\"*40)\n",
    "            print(\"The loss in iteration \",i, \" is \",loss)\n",
    "    \n",
    "    # Plot for loss curve, no need to change\n",
    "    plt.xlabel(\"# iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_his)\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "def predict(w, X):\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4d. Make predictions\n",
    "Use the code above to train your model by calculating the weights for your model and making predictions for the test data.\n",
    "\n",
    "Hint: Compare your training loss with the one using `opt.fmin_bfgs`, your resulting loss should be better or roughly equal to the one computed by inbuilt functions.\n",
    "This is not easy to train, please restart and potentially tune the hyperparameters if youre loss diverge or you get a bad generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = None  # TODO\n",
    "\n",
    "# Codes for comparing your loss and bfgs loss\n",
    "print('Your training loss is:')\n",
    "print(cost(w_train, X_train, t_train, 0.1, X_train.shape[0]))\n",
    "w_train_bfgs = opt.fmin_bfgs(\n",
    "        f=cost, fprime=batch_grad, x0=np.random.normal(scale=0.2, size=(X_train.shape[1],)), args=(X_train, t_train, 0.1, X_train.shape[0]), disp=0)\n",
    "print('The training loss using opt.fmin_bfgs is:')\n",
    "print(cost(w_train_bfgs, X_train, t_train, 0.1, X_train.shape[0]))\n",
    "\n",
    "t_test_pred = None  # TODO\n",
    "\n",
    "# Codes for displaying the results\n",
    "print('The predictions are:')\n",
    "print(t_test_pred)\n",
    "print('The trained parameters are:')\n",
    "print(w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Performance measure\n",
    "\n",
    "There are many ways to compute the performance of a binary classifier. The key concept is the idea of a confusion matrix:\n",
    "\n",
    "|     &nbsp;         | &nbsp;  | Label | &nbsp;  |\n",
    "|:-------------:|:--:|:-----:|:--:|\n",
    "|     &nbsp;         |  &nbsp;  |  0    | 1  |\n",
    "|**Prediction**| 0  |    TN | FN |\n",
    "|      &nbsp;        | 1  |    FP | TP |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions:\n",
    "\n",
    "- `confusion_matrix(y_true, y_pred)`, which returns the confusion matrix as a list of lists given a list of true labels and a list of predicted labels;\n",
    "- `accuracy(cm)`, which takes a confusion matrix and returns the accuracy; and\n",
    "- `balanced_accuracy(cm)`, which takes a confusion matrix and returns the balanced accuracy.\n",
    "\n",
    "The accuracy is defined as $\\frac{TP + TN}{n}$, where $n$ is the total number of examples. The balanced accuracy is defined as $\\frac{1}{2}\\left(\\frac{TP}{P} + \\frac{TN}{N}\\right)$, where $T$ and $N$ are the total number of positive and negative examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "def confusion_matrix(t_true, t_pred):\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def accuracy(cm):\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError\n",
    "def balanced_accuracy(cm):\n",
    "    \n",
    "    #TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs balanced accuracy\n",
    "\n",
    "What is the purpose of balanced accuracy? When might you prefer it to accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your solution here (you can convert this block to markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code which computes the accuracy and balanced accuracy. Discuss the results. (Your accuaray shoud be better than `[0.75,0.5]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(t_test, t_test_pred)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussion of results (compare these two metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Looking back at the prediction task\n",
    "\n",
    "Based on your results, what feature of the dataset is most useful for determining the income level? What feature is least useful? Why?\n",
    "\n",
    "Hint: take a look at ```w_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook Questions (Optional)\n",
    "These questions are hand picked to both be of reasonable difficulty and demonstrate what you are expected to be able to solve. The questions are labelled in Bishop as either $\\star$, $\\star\\star$, or $\\star\\star\\star$ to rate its difficulty.\n",
    "\n",
    "- **Question 4.4**: If you are unfamiliar with lagrange multipliers, look at Appendix E of the textbook. (Difficulty $\\star$, simple algebraic derivation)\n",
    "- **Question 4.5**: (Difficulty $\\star$, simple algebraic derivation)\n",
    "- **Question 1.24**: Note that in the equation $L_{kj}=1-I_{kj}$, $I$ is the identity matrix, so if $k=j$ then $I_{kj}=1$ and $L_{kj}=1-1=0$. (Difficulty $\\star\\star$, requires good understanding of the formulation of how to minimise expected loss)\n",
    "- **Question 1.25**: This requires calculus of variations (used much more later in the course), which is in Appendix D, specifically the Euler-Lagrange result. Assume that everything is continuous and continuously differentiable so that you can bring the differentiation inside the integral sign. (Difficulty $\\star$, simple extension of proof in textbook to multiple target variables)\n",
    "- **Question 4.9**: First state the likelihood. When maximising this, what constraints need to be set? Given such constraints, use lagrange multipliers to derive the results. (Difficulty $\\star$, simple algebraic derivation)\n",
    "- **Question 4.10**: For the covariance matrix, you should be able to only use identities from [Sam Roweis' Matrix Identities](https://cs.nyu.edu/~roweis/notes/matrixid.pdf) to derive the result. Note you can use the cyclic property on $$(x_n-\\mu_k)^T\\Sigma^{-1}(x_n-\\mu_k)$$ as it is a square matrix (scalar). (Difficulty $\\star\\star$, covariance matrix derivation requires uncommon identities)\n",
    "- **Question 4.11**: (Difficulty $\\star\\star$, short derivation but requires understanding what the question setup allows you to apply)\n",
    "- **Question 4.12**: (Difficulty $\\star$, simple algebraic derivation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
